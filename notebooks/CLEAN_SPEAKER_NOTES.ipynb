{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96d04f0",
   "metadata": {},
   "source": [
    "## Probability vs Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffcb01",
   "metadata": {},
   "source": [
    "## Speaker Notes\n",
    "\n",
    "- Emphasize the distinction: probability asks \"what data would we see?\" while likelihood asks \"what parameters explain the data we saw?\"\n",
    "\n",
    "- In the probability case, $p$ is fixed (we know the coin is fair) and we're asking about random outcomes\n",
    "\n",
    "- In the likelihood case, the outcome is fixed (we observed 8/10 tails) and we're searching over possible $p$ values\n",
    "\n",
    "- The likelihood function $\\mathcal{L}(p)$ peaks at $p=0.8$ because that's the parameter value that maximizes the probability of observing exactly what we saw\n",
    "\n",
    "- This is why MLE gives $\\hat{p} = 8/10 = 0.8$ - it's simply the empirical frequency\n",
    "\n",
    "- Note that while $p=0.8$ is most likely, a fair coin ($p=0.5$) could still produce this outcome, just less frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d287456",
   "metadata": {},
   "source": [
    "## MLE or Maximum Likelihood Estimation\n",
    "\n",
    "**Which parameter maximizes the probability of the observed data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d64dc5",
   "metadata": {},
   "source": [
    "## Log-Likelihood\n",
    "\n",
    "Warum?\n",
    "\n",
    "The likelihood is the product of densities:\n",
    "\n",
    "**SAY: \"For 40 data points, you're multiplying 40 numbers each less than 1. This gets astronomically small — computers can't handle it.\"**\n",
    "\n",
    "### Underflow: The Technical Details\n",
    "\n",
    "| Type | Bits | Smallest Number |\n",
    "|------|------|-----------------|\n",
    "| float32 | 32 (1 sign, 8 exp, 23 mantissa) | ~10⁻³⁸ |\n",
    "| float64 | 64 (1 sign, 11 exp, 52 mantissa) | ~10⁻³⁰⁸ |\n",
    "\n",
    "**Why not just use float64?**\n",
    "- 2× memory, 2× slower on GPUs\n",
    "- Deep learning uses float32 or even float16\n",
    "- Still fails: n=1000 → likelihood can be 10⁻⁵⁰⁰\n",
    "\n",
    "**SAY: \"Likelihood shrinks exponentially with sample size. No floating point format saves you.\"**\n",
    "\n",
    "**Solution:** Log turns products into sums:\n",
    "\n",
    "$$\\ell(\\mu, \\sigma) = \\log L = \\sum_{i=1}^{n} \\log f(x_i \\mid \\mu, \\sigma)$$\n",
    "\n",
    "**SAY: \"Log is strictly increasing, so maximizing log-likelihood gives us the same answer as maximizing likelihood.\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b39b8",
   "metadata": {},
   "source": [
    "## MLE Derivation for Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509301c1",
   "metadata": {},
   "source": [
    "## Speaker Notes\n",
    "\n",
    "**MLE and Loss Functions:**\n",
    "- The assumed error distribution determines the loss function: Gaussian → MSE (mean), Laplace → MAE (median)\n",
    "- Both distributions are symmetric (mean = median = mode in population), but MLE mechanics differ due to their probability densities\n",
    "- Gaussian penalizes quadratic deviations → sample mean minimizes MSE\n",
    "- Laplace penalizes linear deviations → sample median minimizes MAE\n",
    "- Using median for Gaussian data is valid but statistically less efficient than MLE\n",
    "\n",
    "**Sample Statistics:**\n",
    "- Population parameter $\\mu$ is unknown; we estimate it with sample statistic $\\bar{x} = \\frac{1}{n}\\sum x_i$\n",
    "- **LLN:** $\\bar{x} \\to \\mu$ as $n \\to \\infty$ (consistency)\n",
    "- **CLT:** Distribution of $\\bar{x}$ approaches $\\mathcal{N}(\\mu, \\sigma^2/n)$ regardless of original distribution\n",
    "- For small samples from Normal: $\\bar{x} \\approx \\text{median} \\approx \\text{mode}$, but not exact equality\n",
    "- For skewed distributions: mean ≠ median even in population (e.g., exponential, log-normal)\n",
    "\n",
    "**Practice:**\n",
    "1. Visualize data distribution (histograms, Q-Q plots)\n",
    "2. Make distributional assumption based on data characteristics\n",
    "3. Choose loss function matching assumption (MSE for Gaussian, MAE for Laplace/outliers)\n",
    "4. Remember: MLE under Gaussian assumption gives $\\hat{\\mu}_{\\text{MLE}} = \\bar{x}$, which is exactly what linear regression with MSE does\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf71c5",
   "metadata": {},
   "source": [
    "### Why Smaller Residuals → Bigger Likelihood\n",
    "\n",
    "From the log-likelihood:\n",
    "\n",
    "$$\\ell = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - w^T x_i)^2$$\n",
    "\n",
    "The sum of squared residuals appears with a **negative coefficient**: $-\\frac{1}{2\\sigma^2}$\n",
    "\n",
    "When $\\sum (y_i - w^T x_i)^2$ is **smaller**:\n",
    "- The term $-\\frac{1}{2\\sigma^2}\\sum (y_i - w^T x_i)^2$ becomes **less negative** (closer to 0)\n",
    "- Therefore $\\ell$ becomes **larger** (maximized)\n",
    "\n",
    "**Example:**\n",
    "- If $\\sum (y_i - w^T x_i)^2 = 100$: $\\ell = \\text{const} - 50$ \n",
    "- If $\\sum (y_i - w^T x_i)^2 = 10$: $\\ell = \\text{const} - 5$ ✓ (larger!)\n",
    "\n",
    "Small errors → high probability of observing data → high likelihood!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d7711",
   "metadata": {},
   "source": [
    "### Note on Bias\n",
    "\n",
    "The MLE estimator $\\hat{\\sigma}^2$ divides by $n$, not $n-1$.\n",
    "\n",
    "- MLE is **biased**: $E[\\hat{\\sigma}^2] = \\frac{n-1}{n}\\sigma^2$\n",
    "- The unbiased estimator uses $n-1$ (Bessel's correction)\n",
    "\n",
    "For large $n$, the difference is negligible. MLE optimizes likelihood, not unbiasedness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd1c20",
   "metadata": {},
   "source": [
    "## Speaker Notes: MLE Bias and Bessel's Correction\n",
    "\n",
    "**Why MLE is biased:**\n",
    "- MLE gives $\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum(x_i - \\bar{x})^2$\n",
    "- The true expected value is $E[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2$, which is slightly less than $\\sigma^2$\n",
    "- This happens because we use sample mean $\\bar{x}$ instead of true mean $\\mu$, which reduces variability\n",
    "\n",
    "**Bessel's correction:**\n",
    "- Unbiased estimator: $s^2 = \\frac{1}{n-1}\\sum(x_i - \\bar{x})^2$\n",
    "- We \"lose one degree of freedom\" by estimating $\\mu$ from data\n",
    "- Now $E[s^2] = \\sigma^2$ (unbiased)\n",
    "\n",
    "**In practice:**\n",
    "- For $n=100$: bias is $\\frac{99}{100} = 0.99$ (1% difference)\n",
    "- For $n=1000$: bias is $\\frac{999}{1000} = 0.999$ (0.1% difference)\n",
    "- Most ML uses large datasets, so the bias is negligible\n",
    "- MLE maximizes likelihood (not unbiasedness), which is why it divides by $n$\n",
    "\n",
    "**Key insight:** MLE doesn't care about unbiasedness - it only cares about what parameter values make the observed data most probable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7680d",
   "metadata": {},
   "source": [
    "## Speaker Notes: Where $E[s^2] = \\sigma^2$ Comes From\n",
    "\n",
    "**The math:**\n",
    "\n",
    "Starting with $\\sum(x_i - \\bar{x})^2$, it can be shown that:\n",
    "\n",
    "$$E\\left[\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = (n-1)\\sigma^2$$\n",
    "\n",
    "This is because:\n",
    "- We have $n$ observations\n",
    "- But $\\bar{x}$ is calculated from the same data, creating a constraint\n",
    "- Only $(n-1)$ values are \"free\" to vary (degrees of freedom)\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = E[s^2] = \\sigma^2$$\n",
    "\n",
    "**Why $n-1$?** When we estimate $\\mu$ with $\\bar{x}$, we've \"used up\" one piece of information from our data. The last observation is determined once you know the first $(n-1)$ and the mean.\n",
    "\n",
    "**Contrast with MLE:**\n",
    "\n",
    "$$E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = \\frac{n-1}{n}\\sigma^2 < \\sigma^2 \\text{ (biased)}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25150b",
   "metadata": {},
   "source": [
    "## Derivation: Why $E[\\sum(x_i - \\bar{x})^2] = (n-1)\\sigma^2$\n",
    "\n",
    "**Start with the sum of squared deviations from sample mean:**\n",
    "\n",
    "$$\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
    "\n",
    "**Trick: Add and subtract the true mean $\\mu$:**\n",
    "\n",
    "$$\\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{n}[(x_i - \\mu) - (\\bar{x} - \\mu)]^2$$\n",
    "\n",
    "**Expand the square:**\n",
    "\n",
    "$$= \\sum_{i=1}^{n}[(x_i - \\mu)^2 - 2(x_i - \\mu)(\\bar{x} - \\mu) + (\\bar{x} - \\mu)^2]$$\n",
    "\n",
    "**Split into three sums:**\n",
    "\n",
    "$$= \\sum_{i=1}^{n}(x_i - \\mu)^2 - 2(\\bar{x} - \\mu)\\sum_{i=1}^{n}(x_i - \\mu) + \\sum_{i=1}^{n}(\\bar{x} - \\mu)^2$$\n",
    "\n",
    "**Simplify middle term:**\n",
    "\n",
    "$$\\sum_{i=1}^{n}(x_i - \\mu) = \\sum_{i=1}^{n}x_i - n\\mu = n\\bar{x} - n\\mu = n(\\bar{x} - \\mu)$$\n",
    "\n",
    "So: $-2(\\bar{x} - \\mu) \\cdot n(\\bar{x} - \\mu) = -2n(\\bar{x} - \\mu)^2$\n",
    "\n",
    "**Simplify last term:**\n",
    "\n",
    "$$\\sum_{i=1}^{n}(\\bar{x} - \\mu)^2 = n(\\bar{x} - \\mu)^2$$\n",
    "\n",
    "**Combine:**\n",
    "\n",
    "$$\\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{n}(x_i - \\mu)^2 - 2n(\\bar{x} - \\mu)^2 + n(\\bar{x} - \\mu)^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n}(x_i - \\mu)^2 - n(\\bar{x} - \\mu)^2$$\n",
    "\n",
    "**Take expectation of both sides:**\n",
    "\n",
    "$$E\\left[\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = E\\left[\\sum_{i=1}^{n}(x_i - \\mu)^2\\right] - E[n(\\bar{x} - \\mu)^2]$$\n",
    "\n",
    "**First term:**\n",
    "\n",
    "$$E\\left[\\sum_{i=1}^{n}(x_i - \\mu)^2\\right] = n \\cdot E[(x_i - \\mu)^2] = n\\sigma^2$$\n",
    "\n",
    "**Second term (using $\\text{Var}(\\bar{x}) = \\frac{\\sigma^2}{n}$):**\n",
    "\n",
    "$$E[n(\\bar{x} - \\mu)^2] = n \\cdot \\text{Var}(\\bar{x}) = n \\cdot \\frac{\\sigma^2}{n} = \\sigma^2$$\n",
    "\n",
    "**Final result:**\n",
    "\n",
    "$$E\\left[\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n",
    "\n",
    "**Therefore:**\n",
    "\n",
    "$$E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = \\sigma^2 \\quad \\text{(unbiased)}$$\n",
    "\n",
    "$$E\\left[\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right] = \\frac{n-1}{n}\\sigma^2 \\quad \\text{(biased)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd25314",
   "metadata": {},
   "source": [
    "**Why MSE uses $\\frac{1}{n}$, not $\\frac{1}{n-1}$:**\n",
    "\n",
    "1. **MSE is for optimization, not estimation:**\n",
    "   - We're minimizing $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$ to find best $w$\n",
    "   - The $\\frac{1}{n}$ is just for averaging—it doesn't affect which $w$ minimizes it\n",
    "   - Scaling by $\\frac{1}{n}$ vs $\\frac{1}{n-1}$ doesn't change the optimal $w$\n",
    "\n",
    "2. **We're not estimating population variance:**\n",
    "   - In statistics, we estimate $\\sigma^2$ from a sample → use $n-1$ for unbiasedness\n",
    "   - In ML, we're minimizing training error → just want average loss per sample\n",
    "\n",
    "3. **Practical reason:**\n",
    "   - $\\frac{1}{n}$ is the true average loss per sample\n",
    "   - Makes metrics comparable across different dataset sizes\n",
    "   - $\\text{MSE} = 0.5$ means \"average squared error is 0.5 per example\"\n",
    "\n",
    "**Bottom line:** Bessel's correction matters when you're doing statistical inference (estimating $\\sigma^2$). For loss functions, we just want the mean—bias doesn't matter because we're optimizing, not estimating population parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e701e",
   "metadata": {},
   "source": [
    "## While training with MSE, we're minimizing the **empirical variance of residuals** (on our training data), which under the Gaussian assumption corresponds to maximizing likelihood.\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - w^T x_i)^2$$\n",
    "\n",
    "This is:\n",
    "- The sample variance of residuals (if mean residual ≈ 0)\n",
    "- MLE estimate $\\hat{\\sigma}^2$ under Gaussian noise assumption\n",
    "- NOT estimating the true population variance $\\sigma^2$ (that would need $n-1$)\n",
    "\n",
    "**What we're really doing:**\n",
    "- Finding $w$ that makes residuals as small as possible\n",
    "- Equivalently: minimizing the spread/variance of prediction errors\n",
    "- Under Gaussian assumption: maximizing likelihood\n",
    "\n",
    "**So yes, you can say:** \"MSE training minimizes the variance of residuals on the training set.\"\n",
    "\n",
    "But remember: it's the *empirical* variance (biased estimator), not an unbiased estimate of population variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8805062",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
