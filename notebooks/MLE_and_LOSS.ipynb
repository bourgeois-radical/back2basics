{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a5415b",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "```\n",
    "1. Problem Definition\n",
    "   └─ Regression? Classification? Time series?\n",
    "\n",
    "2. Data Collection\n",
    "   └─ Sources, APIs, databases\n",
    "\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "   ├─ Distributions\n",
    "   ├─ Correlations\n",
    "   ├─ Missing values\n",
    "   └─ Outliers\n",
    "\n",
    "4. Feature Engineering\n",
    "   ├─ Transformations (log, sqrt)\n",
    "   ├─ Interactions\n",
    "   ├─ Encoding (one-hot, target)\n",
    "   └─ Domain-specific features\n",
    "\n",
    "5. Train/Val/Test Split\n",
    "   └─ Respect time order if applicable\n",
    "\n",
    "6. Preprocessing\n",
    "   ├─ Scaling/Normalization (fit on train only!)\n",
    "   ├─ Imputation (fill missing)\n",
    "   └─ Encoding\n",
    "\n",
    "7. Model Selection\n",
    "   ├─ Start simple (linear, logistic)\n",
    "   ├─ Try trees (Random Forest, XGBoost)\n",
    "   └─ Neural networks if needed\n",
    "\n",
    "!! 8. Training !!\n",
    "   ├─ Choose loss function\n",
    "   ├─ Gradient descent\n",
    "   └─ Monitor train vs. validation\n",
    "\n",
    "9. Hyperparameter Tuning\n",
    "   └─ Cross-validation grid search\n",
    "\n",
    "10. Evaluation\n",
    "    ├─ Multiple metrics\n",
    "    ├─ Residual analysis\n",
    "    └─ Confusion matrix (classification)\n",
    "\n",
    "11. Diagnosis\n",
    "    ├─ Bias (underfitting)? → Add complexity\n",
    "    ├─ Variance (overfitting)? → Regularize\n",
    "    └─ Both? → Get more data\n",
    "\n",
    "12. Final Test\n",
    "    └─ Evaluate on held-out test set\n",
    "\n",
    "13. Deployment\n",
    "    ├─ Model serialization\n",
    "    ├─ API/serving\n",
    "    └─ Monitoring drift\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047e757",
   "metadata": {},
   "source": [
    "# Two approaches to ML: Linear algebra and Statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da94891",
   "metadata": {},
   "source": [
    "Statistical seems to be more meaningful. Check Aggarval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5692a",
   "metadata": {},
   "source": [
    "# MLE = Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e42d49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2674d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.mle_widget import run_mle_widget\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114173a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127.4507123  117.92603548 129.71532807 142.84544785 116.48769938\n",
      " 116.48794565 143.68819223 131.51152094 112.95788421 128.13840065\n",
      " 113.04873461 113.0140537  123.62943407  91.30079633  94.12623251\n",
      " 111.56568706 104.80753319 124.71370999 106.37963887  98.81544448\n",
      " 141.98473153 116.61335549 121.01292307  98.62877721 111.83425913\n",
      " 121.66383885 102.73509634 125.63547028 110.99041965 115.62459375\n",
      " 110.97440082 147.78417277 119.79754163 104.13433607 132.33817368\n",
      " 101.68734525 123.13295393  90.60494814 100.07720927 122.95291854]\n"
     ]
    }
   ],
   "source": [
    "# Song tempos (BPM) - your sample\n",
    "tempos = np.random.normal(loc=120, scale=15, size=40)\n",
    "print(tempos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e426c19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d05563",
   "metadata": {},
   "source": [
    "add formula of MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533d7a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e46434761f747d885d8e7a00b18b514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    <div style=\"background:#1e1e1e; padding:12px; border-radius:8px;\\n           …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mle_widget(data=tempos, feature_name=\"Song Tempo (BPM)\", height=600, width=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6270f",
   "metadata": {},
   "source": [
    "**Key insight:** We're modeling the marginal distribution of x. Just the tempos, period. No prediction, no inputs and outputs, just one variable following one distribution. It is not ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fe9f2",
   "metadata": {},
   "source": [
    "# MLE Derivation for Normal Distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We have observations $x_1, x_2, \\ldots, x_n$ assumed to be i.i.d. from $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "The probability density function (PDF) of a single observation is:\n",
    "\n",
    "$$f(x_i \\mid \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Write the Likelihood Function\n",
    "\n",
    "Since observations are independent, the joint density is the product:\n",
    "\n",
    "$$L(\\mu, \\sigma) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Take the Logarithm\n",
    "\n",
    "$$\\ell(\\mu, \\sigma) = \\log L(\\mu, \\sigma) = \\sum_{i=1}^{n} \\log\\left[\\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\right]$$\n",
    "\n",
    "Using $\\log(ab) = \\log a + \\log b$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[\\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\log\\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\right]$$\n",
    "\n",
    "Since $\\log(e^x) = x$ and $\\log(1/a) = -\\log(a)$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[-\\log(\\sigma) - \\log(\\sqrt{2\\pi}) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]$$\n",
    "\n",
    "Since $\\log(\\sqrt{2\\pi}) = \\frac{1}{2}\\log(2\\pi)$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[-\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]$$\n",
    "\n",
    "The first two terms don't depend on $i$, so they sum to $n$ times themselves:\n",
    "\n",
    "$$\\boxed{\\ell(\\mu, \\sigma) = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Find $\\hat{\\mu}$ — Derivative w.r.t. $\\mu$\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}\\left[-n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right]$$\n",
    "\n",
    "The first two terms are constants w.r.t. $\\mu$, so their derivatives are 0:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\cdot \\frac{\\partial}{\\partial \\mu}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Apply chain rule to $(x_i - \\mu)^2$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu}(x_i - \\mu)^2 = 2(x_i - \\mu) \\cdot \\frac{\\partial}{\\partial \\mu}(x_i - \\mu) = 2(x_i - \\mu) \\cdot (-1) = -2(x_i - \\mu)$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left[-2(x_i - \\mu)\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Solve for $\\hat{\\mu}$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0$$\n",
    "\n",
    "Multiply both sides by $\\sigma^2$ (assuming $\\sigma^2 > 0$):\n",
    "\n",
    "$$\\sum_{i=1}^{n} (x_i - \\mu) = 0$$\n",
    "\n",
    "Expand the sum:\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\mu = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i - n\\mu = 0$$\n",
    "\n",
    "Solve for $\\mu$:\n",
    "\n",
    "$$\\boxed{\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\bar{x}}$$\n",
    "\n",
    "**The MLE for $\\mu$ is the sample mean!**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Find $\\hat{\\sigma}$ — Derivative w.r.t. $\\sigma$\n",
    "\n",
    "Let's rewrite the log-likelihood, treating $\\sigma$ as the variable:\n",
    "\n",
    "$$\\ell = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Take the derivative w.r.t. $\\sigma$:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma}\\left[-n\\log(\\sigma)\\right] + \\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{n}{2}\\log(2\\pi)\\right] + \\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right]$$\n",
    "\n",
    "**First term:**\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-n\\log(\\sigma)\\right] = -n \\cdot \\frac{1}{\\sigma} = -\\frac{n}{\\sigma}$$\n",
    "\n",
    "**Second term:** (constant)\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{n}{2}\\log(2\\pi)\\right] = 0$$\n",
    "\n",
    "**Third term:**\n",
    "\n",
    "Let $S = \\sum_{i=1}^{n}(x_i - \\mu)^2$ (a constant w.r.t. $\\sigma$). We need:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{S}{2\\sigma^2}\\right] = -\\frac{S}{2} \\cdot \\frac{\\partial}{\\partial \\sigma}\\left(\\sigma^{-2}\\right)$$\n",
    "\n",
    "Using power rule: $\\frac{d}{d\\sigma}\\sigma^{-2} = -2\\sigma^{-3} = -\\frac{2}{\\sigma^3}$\n",
    "\n",
    "$$= -\\frac{S}{2} \\cdot \\left(-\\frac{2}{\\sigma^3}\\right) = \\frac{S}{\\sigma^3}$$\n",
    "\n",
    "**Combining all terms:**\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Solve for $\\hat{\\sigma}$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$-\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i - \\mu)^2 = 0$$\n",
    "\n",
    "Multiply both sides by $\\sigma^3$:\n",
    "\n",
    "$$-n\\sigma^2 + \\sum_{i=1}^{n}(x_i - \\mu)^2 = 0$$\n",
    "\n",
    "Solve for $\\sigma^2$:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Substituting $\\hat{\\mu} = \\bar{x}$:\n",
    "\n",
    "$$\\boxed{\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$\\boxed{\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "**The MLE for $\\sigma$ is the (biased) sample standard deviation!**\n",
    "\n",
    "---\n",
    "\n",
    "## Note on Bias\n",
    "\n",
    "The MLE estimator $\\hat{\\sigma}^2$ divides by $n$, not $n-1$.\n",
    "\n",
    "- MLE is **biased**: $E[\\hat{\\sigma}^2] = \\frac{n-1}{n}\\sigma^2$\n",
    "- The unbiased estimator uses $n-1$ (Bessel's correction)\n",
    "\n",
    "For large $n$, the difference is negligible. MLE optimizes likelihood, not unbiasedness.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Parameter | MLE Estimator |\n",
    "|-----------|---------------|\n",
    "| $\\mu$ | $\\hat{\\mu} = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ |\n",
    "| $\\sigma^2$ | $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2$ |\n",
    "\n",
    "These are exactly what you discovered with the sliders!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80853bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf19447",
   "metadata": {},
   "source": [
    "# MLE Derivation for Laplace Distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We have observations $x_1, x_2, \\ldots, x_n$ assumed to be i.i.d. from a Laplace distribution with location $\\mu$ and scale $b$.\n",
    "\n",
    "The probability density function (PDF) of a single observation is:\n",
    "\n",
    "$$f(x_i \\mid \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{\\lvert x_i - \\mu \\rvert}{b}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Write the Likelihood Function\n",
    "\n",
    "Since observations are independent, the joint density is the product:\n",
    "\n",
    "$$L(\\mu, b) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, b) = \\prod_{i=1}^{n} \\frac{1}{2b} \\exp\\left(-\\frac{\\lvert x_i - \\mu \\rvert}{b}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Take the Logarithm\n",
    "\n",
    "$$\\ell(\\mu, b) = \\log L(\\mu, b) = \\sum_{i=1}^{n} \\log\\left[\\frac{1}{2b} \\exp\\left(-\\frac{\\lvert x_i - \\mu \\rvert}{b}\\right)\\right]$$\n",
    "\n",
    "Using $\\log(ab) = \\log a + \\log b$, $\\log(e^x) = x$, and $\\log(1/a) = -\\log(a)$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[-\\log(2b) - \\frac{\\lvert x_i - \\mu \\rvert}{b}\\right]$$\n",
    "\n",
    "The first term doesn't depend on $i$, so it sums to $n$ times itself:\n",
    "\n",
    "$$\\boxed{\\ell(\\mu, b) = -n\\log(2b) - \\frac{1}{b}\\sum_{i=1}^{n}\\lvert x_i - \\mu \\rvert}$$\n",
    "\n",
    "**Key observation:** The log-likelihood contains $\\sum_{i=1}^{n}\\lvert x_i - \\mu \\rvert$. Maximizing the log-likelihood means minimizing this sum. This is exactly the **MAE loss**. Hence: MAE = MLE under Laplace assumption.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Find $\\hat{\\mu}$ — Derivative w.r.t. $\\mu$\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}\\left[-n\\log(2b) - \\frac{1}{b}\\sum_{i=1}^{n}\\lvert x_i - \\mu \\rvert\\right]$$\n",
    "\n",
    "The first term is constant w.r.t. $\\mu$, so its derivative is 0:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{b} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mu}\\lvert x_i - \\mu \\rvert$$\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ The Problem: Absolute Value is Not Differentiable at Zero\n",
    "\n",
    "For the Normal distribution, we had $(x_i - \\mu)^2$, which is smooth everywhere. The absolute value $\\lvert x_i - \\mu \\rvert$ has a **kink** at $x_i = \\mu$.\n",
    "\n",
    "Using the limit definition of the derivative at $x = 0$:\n",
    "\n",
    "$$\\lim_{h \\to 0^+} \\frac{\\lvert h \\rvert - 0}{h} = \\frac{h}{h} = +1 \\qquad \\text{vs} \\qquad \\lim_{h \\to 0^-} \\frac{\\lvert h \\rvert - 0}{h} = \\frac{-h}{h} = -1$$\n",
    "\n",
    "The left and right limits disagree. Therefore, the derivative does not exist at zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: The Derivative Where It Exists\n",
    "\n",
    "Away from the kink, we can compute:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu}\\lvert x_i - \\mu \\rvert = \\begin{cases} -1 & \\text{if } x_i > \\mu \\\\ +1 & \\text{if } x_i < \\mu \\\\ \\text{undefined} & \\text{if } x_i = \\mu \\end{cases}$$\n",
    "\n",
    "This can be written compactly using the **sign function**:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu}\\lvert x_i - \\mu \\rvert = -\\text{sign}(x_i - \\mu)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\text{sign}(z) = \\begin{cases} +1 & \\text{if } z > 0 \\\\ \\ \\ 0 & \\text{if } z = 0 \\\\ -1 & \\text{if } z < 0 \\end{cases}$$\n",
    "\n",
    "**Crucial difference from Normal:** The derivative is $\\pm 1$ regardless of how far $x_i$ is from $\\mu$. The derivative \"forgets\" the magnitude and only remembers the **direction** (above or below).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Setting the Derivative to Zero\n",
    "\n",
    "Substituting back:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{b} \\sum_{i=1}^{n} \\left[-\\text{sign}(x_i - \\mu)\\right] = \\frac{1}{b} \\sum_{i=1}^{n} \\text{sign}(x_i - \\mu) = 0$$\n",
    "\n",
    "Since $b > 0$:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\text{sign}(x_i - \\mu) = 0$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Interpreting the Condition — The Counting Argument\n",
    "\n",
    "Each term $\\text{sign}(x_i - \\mu)$ contributes $+1$ if $x_i > \\mu$ and $-1$ if $x_i < \\mu$.\n",
    "\n",
    "For the sum to equal zero, the number of $+1$s must equal the number of $-1$s:\n",
    "\n",
    "$$\\#\\{x_i > \\mu\\} = \\#\\{x_i < \\mu\\}$$\n",
    "\n",
    "**This is the definition of the median!**\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "Consider five observations: $x = \\{2, 5, 7, 12, 100\\}$\n",
    "\n",
    "Let's evaluate $\\sum_{i=1}^{5} \\text{sign}(x_i - \\mu)$ for different values of $\\mu$:\n",
    "\n",
    "| $\\mu$ | Signs: $\\text{sign}(x_i - \\mu)$ for each $x_i$ | Sum | Interpretation |\n",
    "|-------|------------------------------------------------|-----|----------------|\n",
    "| 3 | $\\text{sign}(2-3), \\text{sign}(5-3), \\text{sign}(7-3), \\text{sign}(12-3), \\text{sign}(100-3)$ = $(-1, +1, +1, +1, +1)$ | $+3$ | 1 below, 4 above → move right |\n",
    "| 6 | $(-1, -1, +1, +1, +1)$ | $+1$ | 2 below, 3 above → move right |\n",
    "| 7 | $(-1, -1, 0, +1, +1)$ | $0$ ✓ | 2 below, 2 above → **balanced!** |\n",
    "| 10 | $(-1, -1, -1, +1, +1)$ | $-1$ | 3 below, 2 above → move left |\n",
    "\n",
    "At $\\mu = 7$ (the median), the sum equals zero. The counts are balanced.\n",
    "\n",
    "**Notice:** The outlier $x_5 = 100$ contributes the same $+1$ as $x_4 = 12$. Outliers have no extra influence — this is why the median is robust!\n",
    "\n",
    "Compare to Normal/MSE: the derivative $\\sum(x_i - \\mu)$ at $\\mu = 7$ would be $(2-7) + (5-7) + (7-7) + (12-7) + (100-7) = -5 - 2 + 0 + 5 + 93 = 91 \\neq 0$. The outlier pulls the mean toward 25.2.\n",
    "\n",
    "$$\\boxed{\\hat{\\mu} = \\text{median}(x_1, x_2, \\ldots, x_n)}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Find $\\hat{b}$ — Derivative w.r.t. $b$\n",
    "\n",
    "Starting from:\n",
    "\n",
    "$$\\ell(\\mu, b) = -n\\log(2b) - \\frac{1}{b}\\sum_{i=1}^{n}\\lvert x_i - \\mu \\rvert$$\n",
    "\n",
    "Let $S = \\sum_{i=1}^{n}\\lvert x_i - \\mu \\rvert$. Using $\\log(2b) = \\log 2 + \\log b$:\n",
    "\n",
    "$$\\ell = -n\\log 2 - n\\log b - \\frac{S}{b}$$\n",
    "\n",
    "Take the derivative w.r.t. $b$:\n",
    "\n",
    "**First term:** $\\frac{\\partial}{\\partial b}(-n\\log 2) = 0$\n",
    "\n",
    "**Second term:** $\\frac{\\partial}{\\partial b}(-n\\log b) = -\\frac{n}{b}$\n",
    "\n",
    "**Third term:** $\\frac{\\partial}{\\partial b}\\left(-\\frac{S}{b}\\right) = -S \\cdot (-b^{-2}) = \\frac{S}{b^2}$\n",
    "\n",
    "**Combining:**\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial b} = -\\frac{n}{b} + \\frac{S}{b^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Solve for $\\hat{b}$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$-\\frac{n}{b} + \\frac{S}{b^2} = 0$$\n",
    "\n",
    "Multiply both sides by $b^2$:\n",
    "\n",
    "$$-nb + S = 0 \\implies b = \\frac{S}{n}$$\n",
    "\n",
    "Substituting $\\hat{\\mu} = \\text{median}$:\n",
    "\n",
    "$$\\boxed{\\hat{b} = \\frac{1}{n}\\sum_{i=1}^{n}\\lvert x_i - \\text{median} \\rvert}$$\n",
    "\n",
    "**The MLE for $b$ is the Mean Absolute Deviation from the median (MAD).**\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Normal vs Laplace\n",
    "\n",
    "| Aspect | Normal (MSE) | Laplace (MAE) |\n",
    "|--------|--------------|---------------|\n",
    "| Term in exponent | $(x_i - \\mu)^2$ | $\\lvert x_i - \\mu \\rvert$ |\n",
    "| Derivative w.r.t. $\\mu$ | $2(x_i - \\mu)$ — proportional to distance | $\\pm 1$ — only the sign |\n",
    "| First-order condition | $\\sum(x_i - \\mu) = 0$ | $\\sum \\text{sign}(x_i - \\mu) = 0$ |\n",
    "| Solution type | Arithmetic (add, divide) | Counting (balance above/below) |\n",
    "| Location MLE | $\\hat{\\mu} = \\bar{x}$ (mean) | $\\hat{\\mu} = \\text{median}$ |\n",
    "| Scale MLE | $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum(x_i - \\bar{x})^2$ | $\\hat{b} = \\frac{1}{n}\\sum\\lvert x_i - \\text{median} \\rvert$ |\n",
    "| Robustness | Sensitive to outliers | Robust to outliers |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "The choice of loss function encodes a distributional assumption about errors:\n",
    "\n",
    "| Loss | Distribution | Optimal Estimator |\n",
    "|------|--------------|-------------------|\n",
    "| MSE | Normal | Mean |\n",
    "| MAE | Laplace | Median |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217a100",
   "metadata": {},
   "source": [
    "### So Why Does This Matter?\n",
    "\n",
    "When we assume $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and derive the MLE, we get MSE, which targets the mean.\n",
    "\n",
    "When we assume $\\varepsilon \\sim \\text{Laplace}(0, b)$ and derive the MLE, we get MAE, which targets the median.\n",
    "\n",
    "For symmetric distributions, mean = median, so both estimators target the same value. But they behave differently with outliers:\n",
    "\n",
    "- **MSE** heavily penalizes large errors (squared), so it's sensitive to outliers\n",
    "- **MAE** penalizes all errors linearly, so it's more robust to outliers\n",
    "\n",
    "Even though they target the same \"center\" for symmetric distributions, the choice matters for robustness and optimization behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2d8c2",
   "metadata": {},
   "source": [
    "# Keep just one example and add others to reference (maybe create a separate notebook)\n",
    "### Real Production Systems & Research Papers: MAE vs MSE Impact\n",
    "\n",
    "Here are actual documented cases from research and industry where the choice between MAE and MSE had measurable impact.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Kaggle: Allstate Claims Severity (2016)**\n",
    "\n",
    "**Competition**: Predict insurance claim costs\n",
    "\n",
    "**Dataset**:  ~188K claims, most $1K-$10K, but some $50K-$100K+\n",
    "\n",
    "**What happened**:\n",
    "- Competition used **MAE as the evaluation metric** (Mean Absolute Error)\n",
    "- Dataset had heavy-tailed distribution of claim amounts\n",
    "- Top solutions used gradient boosting (XGBoost, LightGBM) with MAE-friendly objectives\n",
    "\n",
    "**Winner's approach** (2nd place - Alexey Noskov):\n",
    "- Used custom loss functions and extensive hyperparameter tuning\n",
    "- Neural nets with careful target transformation to handle skewness\n",
    "- Achieved CV score of 1130.29 / LB score of 1110.69\n",
    "\n",
    "**Key insight**: The competition explicitly chose MAE over MSE because claim severity has a long tail, and they wanted models that performed well on typical claims rather than being pulled toward extreme values.\n",
    "\n",
    "**Sources**:\n",
    "- Competition page: https://www.kaggle.com/c/allstate-claims-severity\n",
    "- Winner interview: https://medium.com/kaggle-blog/allstate-claims-severity-competition-2nd-place-winners-interview-alexey-noskov-f4e4ce18fcfc\n",
    "- Analysis: https://medium.com/nerd-for-tech/a-kaggle-competition-allstate-claims-severity-a32f4635c849\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Uber: ETA Prediction System (DeepETA)**\n",
    "\n",
    "**Problem**: Predict arrival times for rides and food delivery\n",
    "\n",
    "**Published Research**: DeepETA (2022)\n",
    "\n",
    "**What they found**:\n",
    "- **Primary metric is MAE** between predicted ETA and actual arrival time\n",
    "- Previous XGBoost models needed to scale to billions of predictions\n",
    "- Switched to deep learning (Transformer-based architecture)\n",
    "\n",
    "**Key requirement from paper**:\n",
    "> \"Accuracy: Uber's primary metric is the **mean absolute error (MAE)** between predicted ETA and true ETA. The new ML model must improve over the XGBoost model.\"\n",
    "\n",
    "**Why MAE**:\n",
    "- Users care about absolute time differences (5 min late vs 5 min early = same impact)\n",
    "- MSE would over-penalize traffic incident outliers\n",
    "- Business impact: more accurate typical-case ETAs improve user trust\n",
    "\n",
    "**Technical details**:\n",
    "- Processes highest QPS (queries per second) at Uber\n",
    "- Achieves median latency of 3.25ms\n",
    "- Deployed globally for all mobility and delivery predictions\n",
    "\n",
    "**Sources**:\n",
    "- Main paper: https://arxiv.org/pdf/2206.02127 (DeeprETA: An ETA Post-processing System at Scale)\n",
    "- Uber blog: https://www.uber.com/blog/deepeta-how-uber-predicts-arrival-times/\n",
    "- Technical overview: https://codecompass00.substack.com/p/uber-billion-dollar-problem-predicting-eta\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Google/DeepMind: Datacenter Cooling (2016-2018)**\n",
    "\n",
    "**Problem**: Optimize cooling systems in Google data centers\n",
    "\n",
    "**Research**: DeepMind AI for data center optimization\n",
    "\n",
    "**Key finding**:\n",
    "- **40% reduction in cooling energy** using neural networks\n",
    "- 15% reduction in overall PUE (Power Usage Effectiveness)\n",
    "\n",
    "**Technical approach**:\n",
    "- Neural networks with 5 hidden layers, 50 nodes each\n",
    "- Trained on 2 years of monitoring data\n",
    "- 19 normalized input variables → 1 output (PUE)\n",
    "\n",
    "**Note on loss functions**:\n",
    "While the public materials don't explicitly state MAE vs MSE was the deciding factor, the technical paper mentions robustness to sensor anomalies was critical:\n",
    "> \"Robust predictions in the presence of sensor failures and equipment anomalies\"\n",
    "\n",
    "This suggests MAE-like robustness properties were valued (sensor failures are outliers).\n",
    "\n",
    "**Sources**:\n",
    "- DeepMind blog: https://deepmind.google/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/\n",
    "- Safety paper: https://deepmind.google/discover/blog/safety-first-ai-for-autonomous-data-centre-cooling-and-industrial-control/\n",
    "- Technical paper: https://arxiv.org/abs/2211.07357 (Controlling Commercial Cooling Systems Using Reinforcement Learning)\n",
    "- MIT Tech Review: https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Medical: ICU Length of Stay Prediction (MIMIC-III)**\n",
    "\n",
    "**Problem**: Predict ICU stay duration for resource planning\n",
    "\n",
    "**Dataset**: MIMIC-III database, ~40K+ ICU stays\n",
    "\n",
    "**Key findings from multiple studies**:\n",
    "- Most stays: 2-5 days (median ~2.64 days)\n",
    "- Long-tail: Some patients stay 30-60+ days (complex complications)\n",
    "\n",
    "**Empirical results** (from various papers):\n",
    "- Support Vector Regressor achieved lowest **MAE of 2.81 days**\n",
    "- RMSE values typically higher but less interpretable for operations\n",
    "- Classification approach (short vs. long stay) often preferred over regression\n",
    "\n",
    "**Why this matters**:\n",
    "- Hospital staffing optimizes for typical cases\n",
    "- Long-stay patients handled with adaptive protocols\n",
    "- MAE predictions more clinically useful for day-to-day planning\n",
    "\n",
    "**Sources**:\n",
    "- MIMIC-III database paper: https://www.nature.com/articles/sdata201635 (Nature, 2016)\n",
    "- ICU prediction study: https://pmc.ncbi.nlm.nih.gov/articles/PMC8135024/ (uses MAE as primary metric)\n",
    "- Length of stay prediction: https://www.mdpi.com/2075-4418/11/12/2242 (MDPI, 2021)\n",
    "- IEEE study: https://ieeexplore.ieee.org/document/10195011/ (R² 0.86, RMSE 1.2)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Walmart: M5 Forecasting Competition (2020)**\n",
    "\n",
    "**Competition**: Predict 28-day ahead sales for 30,490 time series\n",
    "\n",
    "**Dataset**: \n",
    "- 3,049 products across 10 stores in 3 US states\n",
    "- Hierarchical data with zero-inflation (intermittent sales)\n",
    "- 1,941 days of history\n",
    "\n",
    "**Evaluation metric**: **WRMSSE** (Weighted Root Mean Squared Scaled Error)\n",
    "- Despite using a scaled version of RMSE, the metric addresses intermittency\n",
    "- Scaling makes it more robust to extreme values than pure RMSE\n",
    "- Many winners used **Tweedie loss** (between MSE and MAE) for training\n",
    "\n",
    "**Why standard MSE fails here**:\n",
    "- Many zero-sales days (intermittent demand)\n",
    "- Promotional spikes create outliers\n",
    "- MSE over-forecasts slow-moving items to hedge against spikes\n",
    "\n",
    "**Top solutions**:\n",
    "- Used LightGBM with Tweedie objective (power = 1.1 or 1.2)\n",
    "- Tweedie is a generalization: power=0 → Normal (MSE), power=1 → Poisson, power=2 → Gamma\n",
    "- Winners achieved ~22% improvement over benchmarks\n",
    "\n",
    "**Key quote from literature**:\n",
    "> \"MSE optimization led to systematic over-forecasting of slow-moving items to hedge against occasional spikes\"\n",
    "\n",
    "**Sources**:\n",
    "- Competition page: https://www.kaggle.com/competitions/m5-forecasting-accuracy\n",
    "- Academic paper: https://www.sciencedirect.com/science/article/pii/S0169207021001874 (International Journal of Forecasting, 2022)\n",
    "- Results paper: https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/10/M5_accuracy_competition.pdf\n",
    "- Analysis: https://www.christophenicault.com/post/m5_forecasting_accuracy/\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Wind Power Forecasting (Multiple Studies)**\n",
    "\n",
    "**Problem**: Predict wind farm power output for grid integration\n",
    "\n",
    "**Research**: IEEE and various energy journals (2019-2024)\n",
    "\n",
    "**Key challenges**:\n",
    "- High variability in wind conditions\n",
    "- Zero-inflation during calm periods or maintenance\n",
    "- Extreme weather events create outliers\n",
    "\n",
    "**Findings across multiple papers**:\n",
    "- **MSE commonly used** but creates problems with outliers\n",
    "- Papers exploring robust alternatives: Correntropy loss, Huber loss\n",
    "- Recent work on entropy-based loss functions for extreme values\n",
    "\n",
    "**Example study** (2021):\n",
    "> \"Various wind power forecasting methods have been developed... Most of these techniques are designed based on the **mean square error (MSE) loss**, which are very suitable for the assumption that the error distribution obeys the Gaussian distribution. However, there are **many outliers in real wind power data** due to many uncertain factors such as weather, temperature, and other random factors.\"\n",
    "\n",
    "**Proposed solutions**:\n",
    "- LSTM with Correntropy loss (more robust than MSE)\n",
    "- Hybrid models combining MSE for normal conditions + robust loss for extremes\n",
    "\n",
    "**Why it matters**:\n",
    "- Grid operators need typical-case accuracy for daily operations\n",
    "- Extreme events handled by reserve capacity\n",
    "- Trade-off between accuracy metrics (MAE, RMSE) and operational costs\n",
    "\n",
    "**Sources**:\n",
    "- Correntropy loss paper: https://ideas.repec.org/a/eee/energy/v214y2021ics0360544220320879.html (Energy journal, 2021)\n",
    "- Entropy-based loss: https://ieeexplore.ieee.org/document/10520483/ (IEEE, 2024)\n",
    "- Survey paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC9823194/ (comprehensive review, 2023)\n",
    "- ML/DL comparison: https://pmc.ncbi.nlm.nih.gov/articles/PMC12217728/ (Scientific Reports, 2024)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Uber: Time Series Forecasting with Uncertainty (2017)**\n",
    "\n",
    "**Problem**: Predict extreme events (demand spikes, traffic) with uncertainty quantification\n",
    "\n",
    "**Published Research**: \"Deep and Confident Prediction for Time Series at Uber\"\n",
    "\n",
    "**Key contribution**:\n",
    "- Bayesian Neural Networks for uncertainty quantification\n",
    "- Three types of uncertainty: model uncertainty, inherent noise, model misspecification\n",
    "\n",
    "**Loss function considerations**:\n",
    "- Used Gaussian likelihood (equivalent to MSE) as baseline\n",
    "- But incorporated uncertainty estimates to handle outliers\n",
    "- Prediction intervals more important than point estimates\n",
    "\n",
    "**Business impact**:\n",
    "- Better anomaly detection during holidays/events\n",
    "- Reduced false alarm rates\n",
    "- Improved resource allocation\n",
    "\n",
    "**Sources**:\n",
    "- Uber blog: https://www.uber.com/blog/neural-networks-uncertainty-estimation/ (2017)\n",
    "- Technical paper: https://www.researchgate.net/publication/319525051_Deep_and_Confident_Prediction_for_Time_Series_at_Uber\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: When Did MAE/Robust Losses Actually Win?\n",
    "\n",
    "| Domain | Metric Choice | Measured Impact | Source |\n",
    "|--------|---------------|-----------------|---------|\n",
    "| Allstate Insurance | MAE (competition metric) | Better typical-case predictions | Kaggle |\n",
    "| Uber ETA | MAE (primary metric) | More accurate ETAs, user trust | DeepETA paper |\n",
    "| Google Datacenter | Implicit robustness | 40% cooling cost reduction | DeepMind blog |\n",
    "| ICU Prediction | MAE preferred | Better resource allocation | Multiple papers |\n",
    "| Walmart M5 | WRMSSE + Tweedie | 22% over benchmarks | Academic paper |\n",
    "| Wind Power | Exploring robust losses | Ongoing research | IEEE papers |\n",
    "\n",
    "---\n",
    "\n",
    "## The Pattern\n",
    "\n",
    "**MAE/robust losses win when**:\n",
    "1. **Tail events handled separately** (safety margins, alerts, manual review)\n",
    "2. **Typical-case performance matters most** (user experience, daily operations)\n",
    "3. **Heavy-tailed distributions** (insurance claims, retail sales, power spikes)\n",
    "4. **Zero-inflation present** (intermittent demand, equipment downtime)\n",
    "\n",
    "**MSE still appropriate when**:\n",
    "1. Errors genuinely Gaussian (rare in real data)\n",
    "2. All error magnitudes equally important\n",
    "3. Smooth gradients critical for optimization\n",
    "4. Theory assumes Gaussian noise (classical statistics)\n",
    "\n",
    "**Hybrid approaches emerging**:\n",
    "1. Huber loss (MSE for small errors, MAE for large)\n",
    "2. Quantile regression (predict specific percentiles)\n",
    "3. Tweedie/Poisson losses (for count data with zeros)\n",
    "4. Custom loss functions per business objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57716d16",
   "metadata": {},
   "source": [
    "---\n",
    "## Signal-Noise Perspective\n",
    "\n",
    "**Model:**\n",
    "\n",
    "$$y = f(x; \\theta) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$y | x \\sim \\mathcal{N}(f(x; \\theta), \\sigma^2)$$\n",
    "\n",
    "### What Is Noise? Real Examples\n",
    "\n",
    "Epsilon ($\\varepsilon$) represents everything else that affects the outcome besides your input features.\n",
    "\n",
    "For song popularity, noise could be:\n",
    "\n",
    "- The artist's existing fanbase (a huge factor you didn't measure)\n",
    "- Whether the song went viral on TikTok (random luck)\n",
    "- The music video quality (not in your features)\n",
    "- Current cultural trends (zeitgeist)\n",
    "- Pure measurement error (how popularity is counted)\n",
    "- Genuinely random human preferences (some people just randomly like or dislike things)\n",
    "\n",
    "### The Complete Model\n",
    "\n",
    "So the actual observed popularity is:\n",
    "\n",
    "$$y = \\underbrace{50 + 0.3 \\cdot \\text{tempo} - 0.001 \\cdot \\text{tempo}^2}_{f_{\\text{true}}(\\text{tempo})} + \\underbrace{\\varepsilon}_{\\text{all the other stuff}}$$\n",
    "\n",
    "Where $\\varepsilon$ might be normally distributed with mean zero and variance one hundred:\n",
    "\n",
    "$$\\varepsilon \\sim \\mathcal{N}(0, 100)$$\n",
    "\n",
    "This means that even if two songs have **identical tempo**, their popularity can differ by $\\pm 20$ points just due to all these other factors [**95% confidence interval** (roughly $\\pm 2$ standard deviations)].\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "**The noise is real-world randomness and unmeasured variables.** It's why two datapoints with the same $x$ can have different $y$ values.\n",
    "\n",
    "### What Are We Learning?\n",
    "\n",
    "Not the noise — we're learning $f(x; \\theta)$, the **signal**.\n",
    "\n",
    "#### The noise assumption determines the loss function, \n",
    "but we optimize to find the signal that best explains the data despite the noise. \n",
    "\n",
    "Concretely: we find $f(x; \\theta)$ such that residuals $y_i - f(x_i; \\theta)$ look like samples from $\\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "### Why Does the Noise Assumption Matter?\n",
    "\n",
    "Different noise distributions → different optimal strategies:\n",
    "\n",
    "| Assumption | Loss | Behavior | Learns |\n",
    "|------------|------|----------|--------|\n",
    "| Gaussian | MSE | Sensitive to outliers | Mean of $P(y \\| x)$ |\n",
    "| Laplace | MAE | Robust to outliers | Median of $P(y \\| x)$ |\n",
    "\n",
    "**Example:** For $x=1$ with observations $y = \\{5, 5.1, 4.9, 5.2, 100\\}$:\n",
    "- MSE optimal: $f(1) \\approx 24$ (pulled toward outlier)\n",
    "\n",
    "- MAE optimal: $f(1) \\approx 5.1$ (ignores outlier)\n",
    "\n",
    "The noise assumption determines which \"center\" you're learning.\n",
    "\n",
    "### Diagnostic Check\n",
    "\n",
    "After training, verify residuals match the assumed distribution:\n",
    "\n",
    "**For MSE/Gaussian:**\n",
    "\n",
    "- Histogram of residuals → should look normal\n",
    "\n",
    "- Q-Q plot → should be linear\n",
    "\n",
    "- Residual vs. fitted plot → no patterns\n",
    "\n",
    "#### **If residuals don't match:** model is misspecified:\n",
    "\n",
    "- **Wrong noise assumption** → try different loss (MAE, Huber)\n",
    "\n",
    "- **Insufficient capacity** → model too weak to capture the signal → try stronger model\n",
    "\n",
    "### Machine Learning Regression: Many Variables, Conditional Distribution\n",
    "\n",
    "**Setup:** Now you have input features x equals tempo, duration, loudness and an output y equals popularity. You want to predict y from x.\n",
    "\n",
    "**The probabilistic assumption:**\n",
    "\n",
    "Here's the crucial shift in perspective. You're not modeling the distribution of y alone. You're modeling the distribution of y GIVEN x. Mathematically:\n",
    "\n",
    "$$y \\mid x \\sim \\mathcal{N}(f(x; \\theta), \\sigma^2)$$\n",
    "\n",
    "Read this carefully: \"y, conditional on x, follows a normal distribution with mean f of x with parameters theta and variance sigma-squared.\"\n",
    "\n",
    "What this says:\n",
    "- For each specific value of x, there's a distribution of possible y values\n",
    "- That distribution is centered at f of x with theta (your model's prediction)\n",
    "- The spread of that distribution is sigma-squared (the noise level)\n",
    "\n",
    "**Concrete example:**\n",
    "\n",
    "Suppose x equals tempo equals one hundred twenty beats per minute. Your model might predict f of x with theta equals sixty-five (popularity score of sixty-five). The full model says:\n",
    "\n",
    "$$y \\mid \\text{tempo}=120 \\sim \\mathcal{N}(65, 100)$$\n",
    "\n",
    "This means: for songs with tempo one hundred twenty, the popularity scores are normally distributed around sixty-five with standard deviation ten. Some songs will be at fifty, some at eighty, most near sixty-five.\n",
    "\n",
    "**Different x, different distribution:**\n",
    "\n",
    "For tempo equals one hundred forty, your model might predict f of x with theta equals seventy. Then:\n",
    "\n",
    "$$y \\mid \\text{tempo}=140 \\sim \\mathcal{N}(70, 100)$$\n",
    "\n",
    "For this tempo, popularity is centered at seventy.\n",
    "\n",
    "**The key insight: Your model f of x with theta defines the CENTER of the distribution for each x.**\n",
    "\n",
    "### What About Noise in Classification?\n",
    "\n",
    "You asked: \"there is no noise like in regression?\"\n",
    "\n",
    "Great observation. In regression, we have:\n",
    "\n",
    "$$Y = f(\\mathbf{x}) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "In classification, the \"noise\" is inherent in the Bernoulli sampling process itself. Even if we know $p(\\mathbf{x})$ perfectly, the outcome $Y$ is still random — it's a coin flip with probability $p$. The randomness isn't additive noise; it's the fundamental stochasticity of the binary outcome.\n",
    "\n",
    "Think of it this way: in regression, we predict the mean of $Y$, and the noise creates spread around that mean. In classification, we predict the probability of $Y=1$, and the \"noise\" is the irreducible uncertainty of a probabilistic binary event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865756e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a305e79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25e152a",
   "metadata": {},
   "source": [
    "---\n",
    "# We deal with distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d3564",
   "metadata": {},
   "source": [
    "| Distribution | Description | Loss function assumes about |\n",
    "|-------------|-------------|-----------------------------|\n",
    "| $P(X)$ | Input distribution | Nothing! |\n",
    "| $f: X \\to y$ | Learned function | Not a distribution! |\n",
    "| $P(y - f(X))$ | **Residual distribution** | **THIS is what matters!** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bb996",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2423be9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643344d3",
   "metadata": {},
   "source": [
    "## The Chicken-and-Egg Problem: Does the Loss Function Bias Residuals?\n",
    "\n",
    "**The Question:** We don't know residuals before training — we assume a distribution, choose a loss, train, then check residuals. But the loss function shaped those residuals! How do we know the \"true\" distribution if our choice influences what we get?\n",
    "\n",
    "### The Paradox\n",
    "```\n",
    "Before training: We don't know residuals\n",
    "              ↓\n",
    "We ASSUME a distribution (choose loss function)\n",
    "              ↓\n",
    "We train (optimize parameters)\n",
    "              ↓\n",
    "After training: We CHECK if residuals match assumption\n",
    "\n",
    "BUT: The loss function ITSELF shaped those residuals!\n",
    "```\n",
    "\n",
    "### The Deep Answer: There Is No \"True\" Distribution\n",
    "\n",
    "Residuals are not a property of the data — they're a property of **the model**.\n",
    "\n",
    "$$\\text{Residuals} = \\underbrace{(f_{\\text{true}}(x) - f_{\\theta}(x))}_{\\text{model misspecification}} + \\underbrace{\\varepsilon}_{\\text{true noise}}$$\n",
    "\n",
    "We never observe true noise $\\varepsilon$ separately — only its sum with model error.\n",
    "\n",
    "### Yes, Loss Function Changes Residuals\n",
    "\n",
    "**Same data, different losses → different residuals:**\n",
    "\n",
    "| Loss | Behavior | Resulting Residuals |\n",
    "|------|----------|---------------------|\n",
    "| MSE | Spreads error across all points | Systematic bias, no huge outliers |\n",
    "| MAE | Ignores outliers, fits majority | Most near zero, isolated large outliers |\n",
    "\n",
    "Neither is \"wrong\" — they optimize for different objectives.\n",
    "\n",
    "### So What Do We Actually Do?\n",
    "\n",
    "**It's an iterative refinement process:**\n",
    "\n",
    "1. Make initial assumption (e.g., Gaussian) → choose loss (MSE)\n",
    "2. Train model\n",
    "3. Check residuals — do they match assumption?\n",
    "4. **YES** → assumption was reasonable\n",
    "5. **NO** → revise assumption, try different loss, repeat\n",
    "\n",
    "### What We Check For\n",
    "\n",
    "We're not seeking \"truth\" — we're seeking **consistency**:\n",
    "\n",
    "- Residuals centered at 0\n",
    "- Constant variance across fitted values\n",
    "- No correlation with predictors\n",
    "- Match assumed distribution (Q-Q plot)\n",
    "- No systematic patterns in residual vs. fitted plot\n",
    "\n",
    "### The Practical Answer\n",
    "\n",
    "> \"The choice of loss function implicitly assumes a noise distribution, which influences fitted parameters and resulting residuals. This creates a circular dependency — we can't know the 'true' noise distribution a priori.\n",
    ">\n",
    "> The goal isn't finding 'objective truth' but achieving **consistency** between assumptions, residuals, and predictive performance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e0da6",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We have a true function $f_{\\text{true}}(x)$ that we want to learn.\n",
    "\n",
    "We observe noisy data: $y = f_{\\text{true}}(x) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "We train a model $\\hat{f}(x)$ on a finite training set.\n",
    "\n",
    "Our goal is to understand the expected prediction error:\n",
    "\n",
    "$$\\mathbb{E}[(y - \\hat{f}(x))^2]$$\n",
    "\n",
    "---\n",
    "\n",
    "## What Does $\\mathbb{E}[\\cdot]$ Mean Here?\n",
    "\n",
    "The expectation is taken **over different possible training datasets**.\n",
    "\n",
    "Imagine sampling many training sets from the same distribution, training your model on each, and getting different fitted functions $\\hat{f}_1(x), \\hat{f}_2(x), \\hat{f}_3(x), \\ldots$\n",
    "\n",
    "Then $\\mathbb{E}[\\hat{f}(x)]$ is the **average prediction** at point $x$ across all possible training sets.\n",
    "\n",
    "The model itself is a random variable — it depends on which training data you happened to get.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Decompose the Residual\n",
    "\n",
    "Start with the residual at point $x$:\n",
    "\n",
    "$$y - \\hat{f}(x)$$\n",
    "\n",
    "Since $y = f_{\\text{true}}(x) + \\varepsilon$:\n",
    "\n",
    "$$y - \\hat{f}(x) = \\underbrace{(y - f_{\\text{true}}(x))}_{\\varepsilon \\text{ (noise)}} + \\underbrace{(f_{\\text{true}}(x) - \\hat{f}(x))}_{\\text{model error}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Decompose the Model Error\n",
    "\n",
    "Add and subtract $\\mathbb{E}[\\hat{f}(x)]$:\n",
    "\n",
    "$$f_{\\text{true}}(x) - \\hat{f}(x) = \\underbrace{(f_{\\text{true}}(x) - \\mathbb{E}[\\hat{f}(x)])}_{\\text{Bias}} + \\underbrace{(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x))}_{\\text{Variance term}}$$\n",
    "\n",
    "**Bias:** How far is the average model prediction from the truth?\n",
    "\n",
    "**Variance term:** How far is this particular model from the average model?\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Compute $\\mathbb{E}[(y - \\hat{f}(x))^2]$\n",
    "\n",
    "Substitute $y = f_{\\text{true}}(x) + \\varepsilon$:\n",
    "\n",
    "$$\\mathbb{E}[(y - \\hat{f}(x))^2] = \\mathbb{E}[(\\varepsilon + f_{\\text{true}}(x) - \\hat{f}(x))^2]$$\n",
    "\n",
    "Expand the square:\n",
    "\n",
    "$$= \\mathbb{E}[\\varepsilon^2] + 2\\mathbb{E}[\\varepsilon(f_{\\text{true}}(x) - \\hat{f}(x))] + \\mathbb{E}[(f_{\\text{true}}(x) - \\hat{f}(x))^2]$$\n",
    "\n",
    "The cross-term vanishes because $\\mathbb{E}[\\varepsilon] = 0$ and noise is independent of the model:\n",
    "\n",
    "$$= \\sigma^2 + \\mathbb{E}[(f_{\\text{true}}(x) - \\hat{f}(x))^2]$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Decompose the Model Error Term\n",
    "\n",
    "We need to expand $\\mathbb{E}[(f_{\\text{true}}(x) - \\hat{f}(x))^2]$.\n",
    "\n",
    "Add and subtract $\\mathbb{E}[\\hat{f}(x)]$ inside:\n",
    "\n",
    "$$\\mathbb{E}[(f_{\\text{true}}(x) - \\hat{f}(x))^2] = \\mathbb{E}[(f_{\\text{true}}(x) - \\mathbb{E}[\\hat{f}(x)] + \\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x))^2]$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$B = f_{\\text{true}}(x) - \\mathbb{E}[\\hat{f}(x)] \\quad \\text{(bias, a constant)}$$\n",
    "\n",
    "$$V = \\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x) \\quad \\text{(variance term, random)}$$\n",
    "\n",
    "Expand $(B + V)^2$:\n",
    "\n",
    "$$\\mathbb{E}[(B + V)^2] = B^2 + 2B \\cdot \\mathbb{E}[V] + \\mathbb{E}[V^2]$$\n",
    "\n",
    "Compute $\\mathbb{E}[V]$:\n",
    "\n",
    "$$\\mathbb{E}[V] = \\mathbb{E}[\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - \\mathbb{E}[\\hat{f}(x)] = 0$$\n",
    "\n",
    "The cross-term vanishes, leaving:\n",
    "\n",
    "$$= \\underbrace{(f_{\\text{true}}(x) - \\mathbb{E}[\\hat{f}(x)])^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{Variance}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## The Final Result\n",
    "\n",
    "$$\\boxed{\\mathbb{E}[(y - \\hat{f}(x))^2] = \\sigma^2 + \\text{Bias}^2 + \\text{Variance}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\text{Bias} = f_{\\text{true}}(x) - \\mathbb{E}[\\hat{f}(x)]$$\n",
    "\n",
    "$$\\text{Variance} = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "| Term | Meaning | Can We Reduce It? |\n",
    "|------|---------|-------------------|\n",
    "| $\\sigma^2$ | Irreducible noise in the data | No |\n",
    "| $\\text{Bias}^2$ | Systematic error — wrong on average | Yes — use more flexible models |\n",
    "| $\\text{Variance}$ | Instability across training sets | Yes — use simpler models or more data |\n",
    "\n",
    "The tradeoff: reducing bias typically increases variance, and vice versa.\n",
    "\n",
    "---\n",
    "\n",
    "## Connection to MLE and Regularization\n",
    "\n",
    "**MLE** minimizes training error, which tends to reduce bias aggressively at the cost of increasing variance (overfitting).\n",
    "\n",
    "**Regularization** intentionally increases bias to reduce variance:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MAP}} = \\arg\\min_{\\theta} \\left[ \\text{Loss} + \\lambda \\cdot \\text{Penalty} \\right]$$\n",
    "\n",
    "| Regularization | Prior | Penalty Term |\n",
    "|----------------|-------|--------------|\n",
    "| Ridge (L2) | Gaussian: $P(\\theta) \\sim \\mathcal{N}(0, \\tau^2)$ | $\\lambda \\lVert \\theta \\rVert_2^2$ |\n",
    "| Lasso (L1) | Laplace: $P(\\theta) \\sim \\text{Laplace}(0, b)$ | $\\lambda \\lVert \\theta \\rVert_1$ |\n",
    "\n",
    "Regularization is MAP estimation — MLE with a prior belief that parameters should be small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1837cb1",
   "metadata": {},
   "source": [
    "---\n",
    "# What about Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9f7ca",
   "metadata": {},
   "source": [
    "Normally, we assume Bernoulli distribution.\n",
    "\n",
    "### Cross-entropy (assumes Bernoulli/Categorical):\n",
    "### !! This optimizes for the mode (most likely outcome) via probability matching.\n",
    "\n",
    "#### !! Add the derivatives that prov e that we optimize for mode\n",
    "\n",
    "**MLE objective:** Maximize the likelihood of observed data. For a single Bernoulli observation:\n",
    "\n",
    "$$P(y|\\hat{p}) = \\hat{p}^y \\cdot (1-\\hat{p})^{1-y}$$\n",
    "\n",
    "Log-likelihood (easier to optimize):\n",
    "\n",
    "$$\\log P(y|\\hat{p}) = y \\cdot \\log(\\hat{p}) + (1-y) \\cdot \\log(1-\\hat{p})$$\n",
    "\n",
    "Negative log-likelihood (because we minimize losses):\n",
    "\n",
    "$$-\\log P(y|\\hat{p}) = -\\left[y \\cdot \\log(\\hat{p}) + (1-y) \\cdot \\log(1-\\hat{p})\\right]$$\n",
    "\n",
    "That's exactly the **binary cross-entropy** formula.\n",
    "\n",
    "Alternatives exist:\n",
    "You could treat it as a deterministic mapping and use hinge loss (SVMs)—no probabilistic interpretation at all, just geometric margin maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9ae3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "409e741c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "192b2d2a",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68a1b0",
   "metadata": {},
   "source": [
    "You're doing **MLE** when you just minimize the loss (MSE, MAE, cross-entropy).\n",
    "\n",
    "Here's the connection:\n",
    "\n",
    "**MSE ↔ Gaussian residuals:**\n",
    "$$P(y|x, \\theta) = \\mathcal{N}(f_\\theta(x), \\sigma^2)$$\n",
    "$$-\\log P(y|x,\\theta) \\propto (y - f_\\theta(x))^2$$\n",
    "\n",
    "**MAE ↔ Laplace residuals:**\n",
    "$$P(y|x, \\theta) = \\text{Laplace}(f_\\theta(x), b)$$\n",
    "$$-\\log P(y|x,\\theta) \\propto |y - f_\\theta(x)|$$\n",
    "\n",
    "You're finding $\\theta$ that maximizes $P(\\text{data}|\\theta)$ — that's MLE.\n",
    "\n",
    "---\n",
    "\n",
    "**MAP** comes in when you add a **prior** on the weights $P(\\theta)$:\n",
    "\n",
    "$$\\theta_{\\text{MAP}} = \\arg\\max P(\\theta|\\text{data}) = \\arg\\max P(\\text{data}|\\theta) \\cdot P(\\theta)$$\n",
    "\n",
    "In practice:\n",
    "- **L2 regularization** = Gaussian prior on weights → MAP\n",
    "- **L1 regularization** = Laplace prior on weights → MAP\n",
    "\n",
    "## __So: vanilla training = MLE, regularized training = MAP.__ !!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cd2c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e984b695",
   "metadata": {},
   "source": [
    "---\n",
    "# Why we normally use MSE and cross-entropy losses? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185ddca",
   "metadata": {},
   "source": [
    "Central Limit Theorem: With enough data, many noise distributions approximately behave Gaussian in aggregate\n",
    "\n",
    "What for Bernoulli?\n",
    "\n",
    "When Wrong Loss REALLY Hurts:\n",
    "\n",
    "- Regression with outliers: MSE terrible, MAE much better\n",
    "\n",
    "- Imbalanced classification: Standard cross-entropy bad, __weighted/focal loss better__ !!\n",
    "\n",
    "- Count data (always positive): MSE can predict negatives, Poisson loss respects structure\n",
    "\n",
    "- Financial forecasting: Care about worst-case (95th percentile), not mean → quantile loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe3c9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Everything Connects Now\n",
    "\n",
    "### The Complete Picture\n",
    "\n",
    "1. **Data** = points in multidimensional space $(x_i, y_i)$\n",
    "\n",
    "2. **Model** = function mapping inputs to outputs\n",
    "   - Architecture defines the function class\n",
    "   - Parameters $\\theta$ define the specific function $f(x; \\theta)$\n",
    "\n",
    "3. **Loss** = how we measure prediction quality (derived from assumed distribution)\n",
    "   - MSE $\\leftarrow$ Gaussian noise\n",
    "   - MAE $\\leftarrow$ Laplace noise\n",
    "   - Cross-entropy $\\leftarrow$ Bernoulli/Categorical\n",
    "\n",
    "4. **Training** = finding $\\theta$ that minimizes loss via gradient descent\n",
    "\n",
    "5. **Residuals** = what's left after the model does its job\n",
    "   $$\\underbrace{y - f(x; \\theta)}_{\\text{residual}} = \\underbrace{(f_{\\text{true}}(x) - f(x; \\theta))}_{\\text{bias}} + \\underbrace{\\varepsilon}_{\\text{irreducible noise}}$$\n",
    "   \n",
    "   Should look random if the model is good.\n",
    "\n",
    "6. **Bias-Variance Tradeoff** = complexity vs. stability\n",
    "   $$\\mathbb{E}[(y - \\hat{f}(x))^2] = \\underbrace{\\sigma^2}_{\\text{noise}} + \\underbrace{\\text{Bias}^2}_{\\text{underfitting}} + \\underbrace{\\text{Variance}}_{\\text{overfitting}}$$\n",
    "\n",
    "   | Model | Bias | Variance | Problem |\n",
    "   |-------|------|----------|---------|\n",
    "   | Too simple | High | Low | Underfitting |\n",
    "   | Too complex | Low | High | Overfitting |\n",
    "   | Regularized | ↑ slightly | ↓ significantly | Better tradeoff |\n",
    "\n",
    "7. **Generalization** = does the learned function work on new data?\n",
    "   - Validation set checks this\n",
    "   - Overfitting = memorizing training points (high variance)\n",
    "   - Underfitting = too simple function (high bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331d2da",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "**1. Can we train our model in the best possible way with a wrong loss function?**\n",
    "\n",
    "*No — but it depends on what \"optimal\" means.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2dc00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20b99dd2",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Functions Reference\n",
    "\n",
    "### Regression\n",
    "\n",
    "| Loss | Formula | Assumed Distribution | Optimizes | When to Use |\n",
    "|------|---------|---------------------|-----------|-------------|\n",
    "| MSE | $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$ | Gaussian | Mean | Default choice, no outliers |\n",
    "| MAE | $\\frac{1}{n}\\sum\\|y_i - \\hat{y}_i\\|$ | Laplace | Median | Outliers present, robust predictions |\n",
    "| Huber | MSE if $\\|e\\| \\leq \\delta$, MAE otherwise | Gaussian core, heavy tails | Mean (robust) | Best of both: smooth + robust |\n",
    "| Log-Cosh | $\\sum\\log(\\cosh(\\hat{y}_i - y_i))$ | *task-driven* | Mean (robust) | Smooth alternative to Huber |\n",
    "| Quantile | $(q)\\|e\\|$ if $e \\geq 0$, $(1-q)\\|e\\|$ otherwise | *task-driven* | $q$-th quantile | Prediction intervals, asymmetric costs |\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "| Loss | Formula | Assumed Distribution | Optimizes | When to Use |\n",
    "|------|---------|---------------------|-----------|-------------|\n",
    "| Binary Cross-Entropy | $-[y\\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})]$ | Bernoulli | $P(y=1 \\| x)$ | Default for binary classification |\n",
    "| Hinge | $\\max(0, 1 - y \\cdot \\hat{y})$ | *task-driven* | Margin | SVMs, max-margin classifiers |\n",
    "| Focal | $-\\alpha(1-\\hat{p})^\\gamma \\log(\\hat{p})$ | Bernoulli | $P(y=1 \\| x)$ | Imbalanced datasets, hard examples |\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "| Loss | Formula | Assumed Distribution | Optimizes | When to Use |\n",
    "|------|---------|---------------------|-----------|-------------|\n",
    "| Cross-Entropy | $-\\sum_{c} y_c \\log(\\hat{p}_c)$ | Categorical | $P(y=c \\| x)$ | Default for multiclass |\n",
    "| Label Smoothing CE | $-\\sum_{c} y_c' \\log(\\hat{p}_c)$, $y_c' = (1-\\alpha)y_c + \\alpha/K$ | Categorical | Calibrated $P(y=c \\| x)$ | Reduce overconfidence, improve calibration |\n",
    "\n",
    "\n",
    "**Note:** *task-driven* losses are not derived from MLE of a probability distribution — they're designed to optimize a desired property (robustness, quantiles, margins) rather than assuming how noise is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b67fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2531f315",
   "metadata": {},
   "source": [
    "The confusion arises because in classical statistics, you're fitting a distribution to data. The parameters you estimate are properties of that distribution.\n",
    "In machine learning, you're fitting a function that predicts one variable from another. But you still need probability to quantify uncertainty, so you assume a distribution for the outputs given the predictions. The parameters you estimate are properties of the prediction function, not directly of the distribution.\n",
    "The loss function encodes your distributional assumption about the residuals (for regression) or the outputs (for classification). Mean squared error assumes normal residuals. Cross-entropy assumes Bernoulli or categorical outputs. They're all doing MLE, just with different distributional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b8945",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back2basics (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
