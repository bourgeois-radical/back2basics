{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a5415b",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "```\n",
    "1. Problem Definition\n",
    "   └─ Regression? Classification? Time series?\n",
    "\n",
    "2. Data Collection\n",
    "   └─ Sources, APIs, databases\n",
    "\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "   ├─ Distributions\n",
    "   ├─ Correlations\n",
    "   ├─ Missing values\n",
    "   └─ Outliers\n",
    "\n",
    "4. Feature Engineering\n",
    "   ├─ Transformations (log, sqrt)\n",
    "   ├─ Interactions\n",
    "   ├─ Encoding (one-hot, target)\n",
    "   └─ Domain-specific features\n",
    "\n",
    "5. Train/Val/Test Split\n",
    "   └─ Respect time order if applicable\n",
    "\n",
    "6. Preprocessing\n",
    "   ├─ Scaling/Normalization (fit on train only!)\n",
    "   ├─ Imputation (fill missing)\n",
    "   └─ Encoding\n",
    "\n",
    "7. Model Selection\n",
    "   ├─ Start simple (linear, logistic)\n",
    "   ├─ Try trees (Random Forest, XGBoost)\n",
    "   └─ Neural networks if needed\n",
    "\n",
    "!! 8. Training !!\n",
    "   ├─ Choose loss function\n",
    "   ├─ Gradient descent\n",
    "   └─ Monitor train vs. validation\n",
    "\n",
    "9. Hyperparameter Tuning\n",
    "   └─ Cross-validation grid search\n",
    "\n",
    "10. Evaluation\n",
    "    ├─ Multiple metrics\n",
    "    ├─ Residual analysis\n",
    "    └─ Confusion matrix (classification)\n",
    "\n",
    "11. Diagnosis\n",
    "    ├─ Bias (underfitting)? → Add complexity\n",
    "    ├─ Variance (overfitting)? → Regularize\n",
    "    └─ Both? → Get more data\n",
    "\n",
    "12. Final Test\n",
    "    └─ Evaluate on held-out test set\n",
    "\n",
    "13. Deployment\n",
    "    ├─ Model serialization\n",
    "    ├─ API/serving\n",
    "    └─ Monitoring drift\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047e757",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab5692a",
   "metadata": {},
   "source": [
    "# MLE = Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e42d49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2674d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.mle_widget import run_mle_widget\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114173a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127.4507123  117.92603548 129.71532807 142.84544785 116.48769938\n",
      " 116.48794565 143.68819223 131.51152094 112.95788421 128.13840065\n",
      " 113.04873461 113.0140537  123.62943407  91.30079633  94.12623251\n",
      " 111.56568706 104.80753319 124.71370999 106.37963887  98.81544448\n",
      " 141.98473153 116.61335549 121.01292307  98.62877721 111.83425913\n",
      " 121.66383885 102.73509634 125.63547028 110.99041965 115.62459375\n",
      " 110.97440082 147.78417277 119.79754163 104.13433607 132.33817368\n",
      " 101.68734525 123.13295393  90.60494814 100.07720927 122.95291854]\n"
     ]
    }
   ],
   "source": [
    "# Song tempos (BPM) - your sample\n",
    "tempos = np.random.normal(loc=120, scale=15, size=40)\n",
    "print(tempos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533d7a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57b21d336484f1d892ed8cdf80c25c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    <div style=\"background:#1e1e1e; padding:12px; border-radius:8px;\\n           …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mle_widget(data=tempos, feature_name=\"Song Tempo (BPM)\", height=600, width=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fe9f2",
   "metadata": {},
   "source": [
    "# MLE Derivation for Normal Distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We have observations $x_1, x_2, \\ldots, x_n$ assumed to be i.i.d. from $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "The probability density function (PDF) of a single observation is:\n",
    "\n",
    "$$f(x_i \\mid \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Write the Likelihood Function\n",
    "\n",
    "Since observations are independent, the joint density is the product:\n",
    "\n",
    "$$L(\\mu, \\sigma) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Take the Logarithm\n",
    "\n",
    "$$\\ell(\\mu, \\sigma) = \\log L(\\mu, \\sigma) = \\sum_{i=1}^{n} \\log\\left[\\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\right]$$\n",
    "\n",
    "Using $\\log(ab) = \\log a + \\log b$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[\\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\log\\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\right]$$\n",
    "\n",
    "Since $\\log(e^x) = x$ and $\\log(1/a) = -\\log(a)$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[-\\log(\\sigma) - \\log(\\sqrt{2\\pi}) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]$$\n",
    "\n",
    "Since $\\log(\\sqrt{2\\pi}) = \\frac{1}{2}\\log(2\\pi)$:\n",
    "\n",
    "$$\\ell = \\sum_{i=1}^{n} \\left[-\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]$$\n",
    "\n",
    "The first two terms don't depend on $i$, so they sum to $n$ times themselves:\n",
    "\n",
    "$$\\boxed{\\ell(\\mu, \\sigma) = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Find $\\hat{\\mu}$ — Derivative w.r.t. $\\mu$\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}\\left[-n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right]$$\n",
    "\n",
    "The first two terms are constants w.r.t. $\\mu$, so their derivatives are 0:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\cdot \\frac{\\partial}{\\partial \\mu}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Apply chain rule to $(x_i - \\mu)^2$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu}(x_i - \\mu)^2 = 2(x_i - \\mu) \\cdot \\frac{\\partial}{\\partial \\mu}(x_i - \\mu) = 2(x_i - \\mu) \\cdot (-1) = -2(x_i - \\mu)$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left[-2(x_i - \\mu)\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Solve for $\\hat{\\mu}$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0$$\n",
    "\n",
    "Multiply both sides by $\\sigma^2$ (assuming $\\sigma^2 > 0$):\n",
    "\n",
    "$$\\sum_{i=1}^{n} (x_i - \\mu) = 0$$\n",
    "\n",
    "Expand the sum:\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\mu = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i - n\\mu = 0$$\n",
    "\n",
    "Solve for $\\mu$:\n",
    "\n",
    "$$\\boxed{\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\bar{x}}$$\n",
    "\n",
    "**The MLE for $\\mu$ is the sample mean!**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Find $\\hat{\\sigma}$ — Derivative w.r.t. $\\sigma$\n",
    "\n",
    "Let's rewrite the log-likelihood, treating $\\sigma$ as the variable:\n",
    "\n",
    "$$\\ell = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Take the derivative w.r.t. $\\sigma$:\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\sigma} = \\frac{\\partial}{\\partial \\sigma}\\left[-n\\log(\\sigma)\\right] + \\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{n}{2}\\log(2\\pi)\\right] + \\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\right]$$\n",
    "\n",
    "**First term:**\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-n\\log(\\sigma)\\right] = -n \\cdot \\frac{1}{\\sigma} = -\\frac{n}{\\sigma}$$\n",
    "\n",
    "**Second term:** (constant)\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{n}{2}\\log(2\\pi)\\right] = 0$$\n",
    "\n",
    "**Third term:**\n",
    "\n",
    "Let $S = \\sum_{i=1}^{n}(x_i - \\mu)^2$ (a constant w.r.t. $\\sigma$). We need:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\sigma}\\left[-\\frac{S}{2\\sigma^2}\\right] = -\\frac{S}{2} \\cdot \\frac{\\partial}{\\partial \\sigma}\\left(\\sigma^{-2}\\right)$$\n",
    "\n",
    "Using power rule: $\\frac{d}{d\\sigma}\\sigma^{-2} = -2\\sigma^{-3} = -\\frac{2}{\\sigma^3}$\n",
    "\n",
    "$$= -\\frac{S}{2} \\cdot \\left(-\\frac{2}{\\sigma^3}\\right) = \\frac{S}{\\sigma^3}$$\n",
    "\n",
    "**Combining all terms:**\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Solve for $\\hat{\\sigma}$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$-\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(x_i - \\mu)^2 = 0$$\n",
    "\n",
    "Multiply both sides by $\\sigma^3$:\n",
    "\n",
    "$$-n\\sigma^2 + \\sum_{i=1}^{n}(x_i - \\mu)^2 = 0$$\n",
    "\n",
    "Solve for $\\sigma^2$:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "Substituting $\\hat{\\mu} = \\bar{x}$:\n",
    "\n",
    "$$\\boxed{\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$\\boxed{\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}}$$\n",
    "\n",
    "**The MLE for $\\sigma$ is the (biased) sample standard deviation!**\n",
    "\n",
    "---\n",
    "\n",
    "## Note on Bias\n",
    "\n",
    "The MLE estimator $\\hat{\\sigma}^2$ divides by $n$, not $n-1$.\n",
    "\n",
    "- MLE is **biased**: $E[\\hat{\\sigma}^2] = \\frac{n-1}{n}\\sigma^2$\n",
    "- The unbiased estimator uses $n-1$ (Bessel's correction)\n",
    "\n",
    "For large $n$, the difference is negligible. MLE optimizes likelihood, not unbiasedness.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Parameter | MLE Estimator |\n",
    "|-----------|---------------|\n",
    "| $\\mu$ | $\\hat{\\mu} = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ |\n",
    "| $\\sigma^2$ | $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2$ |\n",
    "\n",
    "These are exactly what you discovered with the sliders!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e152a",
   "metadata": {},
   "source": [
    "---\n",
    "# We always deal with distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d3564",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back2basics (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
